{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN attempt --> changed the loss within the separate loss functions + added weight clipping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries regarding torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde, lognorm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30218\n",
      "30218\n",
      "(30218, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading from RSA_input.csv\n",
    "df = pd.read_csv('RSA_input.csv') # add csv file to the correct location\n",
    "grain_R = df[\"grain_R\"]\n",
    "grain_asp = df[\"aspect\"]\n",
    "print(len(grain_R)) # length 30218\n",
    "print(len(grain_asp)) # length 30218\n",
    "\n",
    "# Combined into numpy shape (30218, 2)\n",
    "grainsData = np.column_stack((grain_R, grain_asp))\n",
    "print(grainsData.shape)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "critic_iterations = 5\n",
    "weight_clip = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = -1.\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "latent_Gaussian_dimension = 100  # Dimension of the input noise vector\n",
    "number_of_grain_features = 2  # Dimension of the real data\n",
    "#real_data_dim = 30218  # Dimension of the real data\n",
    "number_of_reduced_grains = 1000  # Dimension of the generated data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_Gaussian_dimension, number_of_grain_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(latent_Gaussian_dimension, 75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, number_of_grain_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_grain_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(number_of_grain_features, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def generator_loss(discriminator, fake_grains): # This has been changed to the WGAN loss\n",
    "\n",
    "    # Ensure that the target tensor has the same device as the input fake_grains\n",
    "    target = torch.full((fake_grains.size(0),), real_label, device=fake_grains.device)\n",
    "\n",
    "    # Forward pass through the discriminator with the fake grains\n",
    "    output = discriminator(fake_grains).reshape(-1)\n",
    "\n",
    "    # generator loss = -(avgerage critic score on fake images)\n",
    "    loss = -torch.mean(output)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def discriminator_loss(discriminator, real_grains, fake_grains):\n",
    "\n",
    "    '''\n",
    "    d_loss_real: The mean of the binary cross-entropy losses computed on the real_grains.\n",
    "    D_real: Mean output of the discriminator for real_grains. This is useful for tracking convergence.\n",
    "    d_loss_fake: The mean of the binary cross-entropy losses computed on the fake_grains.\n",
    "    D_fake: Mean output of the discriminator for fake_grains. This is useful for tracking convergence.\n",
    "\n",
    "    '''\n",
    "\n",
    "    device = fake_grains.device  # Get the device of the input tensors\n",
    "    # Transfer input tensors to the same device as the discriminator\n",
    "    real_grains = real_grains.to(device)\n",
    "    fake_grains = fake_grains.to(device)\n",
    "\n",
    "    # Create the target labels for real and fake grains\n",
    "    real_target = torch.full((real_grains.size(0),), real_label, device=device)\n",
    "    fake_target = torch.full((fake_grains.size(0),), fake_label, device=device)\n",
    "\n",
    "    # Compute the discriminator outputs for real and fake grains --> critic_real & critic_fake\n",
    "    real_output = discriminator(real_grains).view(-1)\n",
    "    fake_output = discriminator(fake_grains).view(-1)\n",
    "\n",
    "    # Compute the W-losses\n",
    "    # Discrumunator loss =  (avegrage critic score on real images) - (average critic score on fake images)\n",
    "    loss_critic = -(torch.mean(real_output) - torch.mean(fake_output))\n",
    "    d_loss_real = torch.mean(real_output)\n",
    "    d_loss_fake = torch.mean(fake_output)\n",
    "\n",
    "    # Compute the mean discriminator outputs for real and fake grains\n",
    "    D_real = real_output.mean().item()\n",
    "    D_fake = fake_output.mean().item()\n",
    "\n",
    "    return d_loss_real, D_real, d_loss_fake, D_fake, loss_critic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Shapes and Loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5048, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test generator loss\n",
    "def test_generator_loss():\n",
    "    # Create a generator\n",
    "    netG = Generator(latent_Gaussian_dimension=2, number_of_grain_features=2)\n",
    "    netD = Discriminator(number_of_grain_features=2)\n",
    "    # Create fake grains\n",
    "    noise = torch.randn(100, 2)\n",
    "    fake_grains = netG(noise)\n",
    "\n",
    "    # Compute the generator loss \n",
    "    loss = generator_loss(netD, fake_grains)\n",
    "    print(loss)\n",
    "\n",
    "test_generator_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5169, grad_fn=<MeanBackward0>)\n",
      "0.5168965458869934\n",
      "tensor(0.5161, grad_fn=<MeanBackward0>)\n",
      "0.5161435008049011\n",
      "tensor(-0.0008, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test discriminator loss\n",
    "def test_discriminator_loss():\n",
    "    # Create a generator\n",
    "    netG = Generator(latent_Gaussian_dimension=2, number_of_grain_features=2)\n",
    "    netD = Discriminator(number_of_grain_features=2)\n",
    "    # Create real and fake grains\n",
    "    real_grains = torch.randn(100, 2)\n",
    "    noise = torch.randn(100, 2)\n",
    "    fake_grains = netG(noise)\n",
    "    # Compute the discriminator loss\n",
    "    d_loss_real, D_real, d_loss_fake, D_fake, loss_critic = discriminator_loss(netD, real_grains, fake_grains)\n",
    "    print(d_loss_real)\n",
    "    print(D_real)\n",
    "    print(d_loss_fake)\n",
    "    print(D_fake)\n",
    "    print(loss_critic)\n",
    "\n",
    "test_discriminator_loss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Discriminator Loss: D_real=0.5002, D_fake=0.4998, d_loss = -0.0003\n",
      "Generator Loss: -0.4998\n",
      "Epoch 5\n",
      "Discriminator Loss: D_real=0.5001, D_fake=0.5001, d_loss = -0.0000\n",
      "Generator Loss: -0.5001\n",
      "Epoch 10\n",
      "Discriminator Loss: D_real=0.5002, D_fake=0.5002, d_loss = -0.0000\n",
      "Generator Loss: -0.5001\n",
      "Epoch 15\n",
      "Discriminator Loss: D_real=0.5001, D_fake=0.5001, d_loss = 0.0000\n",
      "Generator Loss: -0.5002\n",
      "Epoch 20\n",
      "Discriminator Loss: D_real=0.5004, D_fake=0.5004, d_loss = -0.0000\n",
      "Generator Loss: -0.5004\n",
      "Epoch 25\n",
      "Discriminator Loss: D_real=0.5005, D_fake=0.5005, d_loss = -0.0000\n",
      "Generator Loss: -0.5005\n",
      "Epoch 30\n",
      "Discriminator Loss: D_real=0.5005, D_fake=0.5005, d_loss = -0.0000\n",
      "Generator Loss: -0.5005\n",
      "Epoch 35\n",
      "Discriminator Loss: D_real=0.5005, D_fake=0.5005, d_loss = -0.0000\n",
      "Generator Loss: -0.5005\n"
     ]
    }
   ],
   "source": [
    "# initialize gen and discriminator_loss\n",
    "generator = Generator(latent_Gaussian_dimension, number_of_grain_features).to(device)\n",
    "discriminator = Discriminator(number_of_grain_features).to(device)\n",
    "\n",
    "#initialize optimizer\n",
    "generator_optimizer = optim.RMSprop(generator.parameters(), lr=0.00005)\n",
    "discriminator_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.00005)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(grainsData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize tensorboard plotting\n",
    "\n",
    "# train gen & critic in loop\n",
    "for epoch in range(num_epochs + 1):\n",
    "    for batch_index, real_grains in enumerate(dataloader):\n",
    "        real_grains = real_grains.to(torch.float32)\n",
    "\n",
    "        # Train discriminator = max E(discriminator(real) - E(dicriminator(fake)))\n",
    "        for _ in range(critic_iterations):\n",
    "            noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "            fake_grains = generator(noise)\n",
    "            d_loss_real, D_real, d_loss_fake, D_fake, d_loss = discriminator_loss(discriminator, real_grains, fake_grains)\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Update discriminator\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            # clip critic weigts between -0.01 and 0.01\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-weight_clip, weight_clip)\n",
    "        \n",
    "        # Train Generator: max E(discriminator(fake)) <--> min -E(discriminator(fake))\n",
    "        # i.e. we want discriminator to think that the fake data is real\n",
    "        \n",
    "        # Generate fake grains\n",
    "        noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device)\n",
    "        fake_grains = generator(noise)\n",
    "        \n",
    "        # Compute generator loss\n",
    "        g_loss = generator_loss(discriminator, fake_grains)\n",
    "\n",
    "        # Update generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "    ############################\n",
    "    # Print training progress\n",
    "    ###########################\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Discriminator Loss: D_real={D_real:.4f}, D_fake={D_fake:.4f}, d_loss = {d_loss:.4f}\")\n",
    "        print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
